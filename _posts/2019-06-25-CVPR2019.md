---
title: 'CVPR2019 Paper Reading(Domain Adaptation)'
date: 2019-06-25
permalink: /posts/2012/08/blog-post-14/
tags:
  - CVPR 2019
  - Unsupervised Domain Adaptation
---

赶热点 Domain adaptation... 今年oral做UDA的真多...

+ **Taking A Closer Look at Domain Shift: Category-level Adversaries for Semantics Consistent Domain Adaptation** (郑良，于俊清)

今年华科的oral不少，很感兴趣的一篇文章，方法不复杂且很有insight，这篇主要follow了2018年CVPR spotlight *Learning to adapt structured output space for semantic segmentation*(Ming-Hsuan Yang组)

解决的问题：　目前domain adaptation的方法采用的是global adaptation可能导致一些本来对齐的类别被错误地映射。

idea:　减小类别对齐特征的对抗loss权重，增加没有对齐的类别的对抗loss权重。

目前的方法：1.对齐不同domain的feature，减小两个domain的散度降低目标domain的错误上界；

2.生成对抗：生声器生成特征让判别器难以分辨特征属于哪个domain

目前方法的问题：只考虑全局边界分布，不考虑局部联合分布的偏移。

方法：基于co-training，在原本的adversarial loss中加入weight discrepancy loss和在self-adaptive adversarial loss中加入一个类别的cosine distance矩阵（两个classifier输出的predictions之间的相似性矩阵）。

总的优化目标是：分割的loss（多类交叉熵损失）+　weight discrepancy loss + category-level adversatial loss

其中weight discrepancy loss是两个分类器当中卷积核参数经过flatten和concatenate之后的cosine距离，多类交叉熵损失的输入是将两个分类器输出的predictions加和（ensemble）。

t-SNE可视化特征分布来看，使用CLAN的特征边界更清晰，大多数类别的分割效果比去年TAN好一些。


+ **Transferrable Prototypical Networks for Unsupervised Domain Adaptation** (梅涛，姚霆)

TPN首先将目标样本匹配到source domain当中距离最近的原型，优化方案：端到端训练，最小化source-only, target-only和source-target数据的distance,以及每一对原型的输出分数分布的KL散度。

现有的方法：
1.对齐source domain和target domain的数据分布

2.最小化domain不变性通过最小化shift例如：相关性距离，最大均值差异
