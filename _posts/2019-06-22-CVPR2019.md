---
title: 'CVPR2019 Paper Reading(Part 2)'
date: 2019-06-21
permalink: /posts/2012/08/blog-post-10/
tags:
  - CVPR 2019
---

继续扫文...

+ **Semantics Distangling for Text-to-Image Generation**(王晓刚商汤组) :star::star::star:

提出SD-GAN，大致架构是siamese＋sequential stacked generatror-discriminator modules（其组里之前１７年两篇工作都用了），
用contrastive loss，目的是缩小同一图片两个不同描述生成的图片差异，增大不同图片的差异。

其组里做的AttnGAN和StackGAN处理细粒度的text2img比较成功，但是处理文字表达的多样性还是一个challenge。

**Text Encoder**: 用一个BiLSTM，取最后一个hidden layer的输出作为text的feature vector

**Contrastive loss**: 在普通的contrastive loss上改了一个当y=1时，让d尽量接近alpha而不是０，防止模式坍缩

**Semantic-Conditioned Batch  Normalization(SCBN)**:　在原始的Conditional Batch Norm(CBN ICLR2017)上加入了sentence cues和word cues，
sentence cues加了一个将输入进行MLP变换的操作，word cues将word和视觉特征之间通过MLP关联，得到的semantic feature vector输入到原始的CBN中。

**Experimental Results**:实验结果inception score在CUB和MS-COCO上大幅超过了其组里之前的AttnGAN和StackGAN。实验结果表明SCBN植入到其他网络例如
AttnGAN中能够带来明显的性能增益。

+ **Inserting Videos into Videos**(Ming-Husan Yang组) :star::star:

看起来很有意思的一篇文章，将一个视频中的物体植入到另一个视频当中，直观效果不错，首先能够想到的方法是先做分割。（又是一个新坑？...）

具体方法感兴趣再看，实验结果通过YOLOv3看能否检测到合成的视频中被正确植入了物体，用到一些person re-id的数据集，结果表明该提出的方法precision和recall
值高出现有算法10%左右，合成效果明显优于现有的分割算法DeepLabv3+（tensorflow group）。

+ **STEP: Spatio-Temporal Progressive Learning for Video Action Detection** (Larry Davis组) :star::star::star:

Idea: follow AVA和ICCV2017的ACT Detector 先初始化一个粗尺度的proposal长方体，然后随着action逐渐提炼proposal，然后在每个time step适应性地扩展proposal到来包含更多相关的时序上下文。感觉看起来可以follow的地方很多，方法总的来说是detect+classification并不难，在UCF101和AVA上测试结果还不错。

+ **Unsupervised Visual Domain Adaptation: A Deep Max-Margin Gaussian Process Approach**  :star:

标题入坑，数学渣完全看不懂= =　abstract的一个有趣的statement:　在无监督的domain adaptation，匹配输入的边际分布和对齐输出的分布(predictions)，对齐输出的分布可以通过最小化预测变量的最大差异（减小两个domain的classifier的最大输出差异？）

+ **Shifting More Attention to Video Salient Object Detection** (程明明组)

做video salient object detection(VSOD)的，提出了一个目前largest-scale的visual-attention-consistent Densely Annotated VSOD dataset(DAVSOD)，提出了saliency shift的challenge，总共226 videos和23,938 frames（总共大约84k frames），给了一个baseline: saliency-shift-aware convLSTM，估计之后有一波刷榜。

PS:论文里的图表画的太好了，学习一下

