---
title: 'CVPR2019 Paper Reading'
date: 2019-06-21
permalink: /posts/2012/08/blog-post-9/
tags:
  - CVPR 2019
---

The most suitable approach to handle disappointment is reading papers. :):disappointed:

CVPR2019 Papers:

**Oral:**

:star:  scan

:star::star:  read carefully 

:star::star::star:   read carefully & recommend

+ **Learning Video Representations from Correspondence Proposals.** :star::star::star:

**abstract**: building long-range dependencies, sounds like the same spirit as non-local, attention etc.

**motivation**: idea gains from deep learning on point clouds and point motion.搜了下这几位作者之前是做３ｄ的，正好把相关的idea迁移到视频上。与这篇文章相关的影子：GCN,non-local等。

**interesting points**: 总结了什么是好的视频表示

1. Corresponding positions have similar visual or semantic features.

2. Corresponding positions can span arbitrarily long ranges, spatially or temporally.

3. Potential correspondence positions in other frames are small in percentage.

提出的CP Module->升级版non-local

主要分为两个部分：

1. 将输入的视频特征tensor看成THWC的点云，用k-nn grouping先算出所有点云的THWxk的相似度矩阵。

2. :ghost:Correspondence Embedding Layer: 将相似度矩阵和原始的视频特征之间进行关联，学习到一个具有长时段关联的视频特征

将k个帧进行两两之间的空间偏移度和语义向量关联的学习，训练k个权重共享的MLP，再通过max pooling，输出一个THWC'的特征tensor，为了这个模块易于植入其他模型中，设置C=C'。

实验结果：

1.toy dataset上远远超过其他模型，可以学到视频中一个很小的物体的运动模式。

2.Kinetics上超过了nonlocal c2d net 大约2.5%
