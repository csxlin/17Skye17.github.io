---
title: 'CVPR2019 Paper Reading'
date: 2019-06-21
permalink: /posts/2012/08/blog-post-9/
tags:
  - CVPR 2019
---

The most suitable approach to handle disappointment is reading papers. :):disappointed:

CVPR2019 Papers:

**Oral:**

:star:  scan

:star::star:  read carefully 

:star::star::star:   read carefully & recommend

+ **Learning Video Representations from Correspondence Proposals.** :star::star::star:

**abstract**: building long-range dependencies, sounds like the same spirit as non-local, attention etc.

**motivation**: idea gains from deep learning on point clouds and point motion.搜了下这几位作者之前是做３ｄ的，正好把相关的idea迁移到视频上。与这篇文章相关的影子：GCN,non-local等。

**interesting points**: 总结了什么是好的视频表示

1. Corresponding positions have similar visual or semantic features.

2. Corresponding positions can span arbitrarily long ranges, spatially or temporally.

3. Potential correspondence positions in other frames are small in percentage.

提出的CP Module->升级版non-local

主要分为两个部分：

1. 将输入的视频特征tensor看成THWC的点云，用k-nn grouping先算出所有点云的THWxk的相似度矩阵。

2. :ghost:Correspondence Embedding Layer: 将相似度矩阵和原始的视频特征之间进行关联，学习到一个具有长时段关联的视频特征

将k个帧进行两两之间的空间偏移度和语义向量关联的学习，训练k个权重共享的MLP，再通过max pooling，输出一个THWC'的特征tensor，为了这个模块易于植入其他模型中，设置C=C'。

实验结果：

1.toy dataset上远远超过其他模型，可以学到视频中一个很小的物体的运动模式。

2.Kinetics上超过了nonlocal c2d net 大约2.5%

+ **Detect-to-Retrieve: Efficient Regional Aggregation for Image Search** :star::star:

1.　提出了一个Google Landmark Boxes dataset，一共包含86k的图片，15k个不同landmark类别。 从原始的GLD数据集发展而来（共1.2M的图片，一共15k个不同种类的landmark）

2.　基于Deep local features(DELF)和aggregated selective match kernels(ASMK)提出了R-ASMK。

目前看完觉得算法并没有特别创新，主要是在原来的ASMK和AMK的基础上，把对于整张图片的聚合后的描述子的操作用到了对region上，region　search从原来的计算２张图片的distance变成计算２张图片的region之间的distance再pooling, VLAD和ASMK相似度可以用局部聚合描述子来计算表示。对于一个给定的图片区域，绝大多数视觉词与任何局部特征都无关，对于该图片区域的残差都是０，如果视觉特征只在图片中一小部分出现的话，就会导致得到的残差值偏低（VLAD的误差变大）

实验结果看来很work，内存占用只有不到SOTA的一半。

主要的贡献在于数据集吧...

+ **Video Generation from Single Semantics Label Map** 港中文王晓刚组 :star:

由一张语义标签map生成一段连续的视频帧，方法分为两步:

1. 由第一帧转换成一张image：seg2image

2. 由cVAE从第一张image生成连续的视频帧：image2video

相关算法：

1. pix2pix　　单张图片从seg2image

2. vid2vid  　多帧seg生成多视频帧

3. seg2vid    单帧seg生成多帧视频

具体方法：

1. Image-to-Image(I2I):用现有的性能最好的图片翻译模型pix2pixHD从seg翻译成image（可用其他方法替代）

2. Image-to-Video(I2V):包含两部分flow prediction和video frame generation from flow.

Conditional VAE(cVAE，16年NIPS)　先留着感兴趣再看

+ ****
