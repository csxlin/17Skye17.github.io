---
title: 'Video Architecture Search '
date: 2019-11-12
permalink: /posts/2012/08/blog-post-24/
tags:
  - video model
  - NAS
---

From Google AI Blog

Video understanding is a challenging problem. Because a video contains spatio-temporal data, its feature representation is required to abstract both appearance and motion information. This is not only essential for automated understanding of the semantic content of videos, such as web-video classification or sport activity recognition, but is also crucial for robot perception and learning. Just like humans, an input from a robot’s camera is seldom a static snapshot of the world, but takes the form of a continuous video. The abilities of today’s deep learning models are greatly dependent on their neural architectures. Convolutional neural networks (CNNs) for videos are normally built by manually extending known 2D architectures such as Inception and ResNet to 3D or by carefully designing two-stream CNN architectures that fuse together both appearance and motion information. However, designing an optimal video architecture to best take advantage of spatio-temporal information in videos still remains an open problem. Although neural architecture search (e.g., Zoph et al, Real et al) to discover good architectures has been widely explored for images, machine-optimized neural architectures for videos have not yet been developed. Video CNNs are typically computation- and memory-intensive, and designing an approach to efficiently search for them while capturing their unique properties has been difficult. In response to these challenges, we have conducted a series of studies into automatic searches for more optimal network architectures for video understanding. We showcase three different neural architecture evolution algorithms: learning layers and their module configuration (EvaNet); learning multi-stream connectivity (AssembleNet); and building computationally efficient and compact networks (TinyVideoNet). The video architectures we developed outperform existing hand-made models on multiple public datasets by a significant margin, and demonstrate a 10x~100x improvement in network runtime.


1. EvaNet:从网络层和模块上搜索最优参数

2. AssembleNet:搜索多流之间的连接方式

3. TinyVideoNet:每个大约1s的video clip只需要大约10ms的GPU时间


**Ref:**

Evolving Space-Time Neural Architectures for Videos (ICCV2019)

AssembleNet: Building stronger and better (multi-stream) models （目测ECCV, Charades上性能超slowfast 13个点）

Tiny Video Networks: The fastest video understanding networks(目测CVPR)
