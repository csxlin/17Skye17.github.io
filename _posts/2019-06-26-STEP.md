---
title: 'CVPR2019 Paper Reading (STEP)'
date: 2019-06-26
permalink: /posts/2012/08/blog-post-16/
tags:
  - CVPR 2019
  - Video Action Detection
---

应用类扫文...

**STEP: Spatio-Temporal Progressive Learning for Video Action Detection** (Larry Davis组)

Abstract: Spatio-TEmporal Progressive action detector首先在视频序列上定义一些粗尺度的长方体，然后经过几个step逐步提炼更准确的长方体。

Introduction:　当前的方法主要是two-stage的framework，首先采用region proposal算法产生action proposal，然后用这些proposal来进行动作分类或者精确定位。（主要沿用的object detection算法，例如Faster R-CNN）。

当前的Challenge:１．随着时间推移，action tube经常会有空间移位，对于proposal的生成和微调带来额外的计算复杂度；２．大量动作只有时间上下文已知的情况下才能被准确分类，因此高效的时序模型非常重要。

当前方法把2D的proposal扩展到3D时（考虑时间变量），假设了空间位置不发生偏移（强假设）

Framework Overview:首先初始化１１个proposal然后进行extend/refine/update。提取特征的backbone是VGG16和I3D。11个proposal是有两个尺度，五个一组，分别在四个角和图片中心，外加一个全图的proposal。Extend：在时序上对proposal逐步扩展；Refine:在空间上对proposal进行提炼;Update:计算回归和分类分数，每一步贪心地选择分数最高的proposal替代原来的proposal。

1. Spatial Refinement

模型分为两个分支，一个采用global特征完成分类，另一个采用local特征（frame-level）完成回归。　分类采用的local特征是原来local特征和global特征拼接的，模型是一个multitask的训练过程，训练loss包括多类交叉熵损失的分类loss＋一个clip上每一帧的bbox L1,smooth的平均值（回归loss）

2. Temporal Extension

每个step向相邻的clip扩展（Ks+2K）。扩展方法：1.外推：如果空间移位是线性变化的，可以用一个线性外推方程表示下一次移位的结果。

2.预测：根据当前clip动作预测相邻clip的动作，采用两个预测回归

模型训练：累计所有step的loss，然后同时反向传播更新整个模型。训练过程有两个问题：
1.分布变化：每一步输入和输出的分布都在变化，因为初始化是一些粗尺度的grid，在refine的过程当中会改变。３个解决方法：1.不同的step前用不同的header；2.在多个step之后增加IoU阈值，初始先用较小的阈值之后再增大；
3.采用hard-aware sampling策略在训练的时候选择更加informative的sample。

Hare-Aware Sampling: 基于两个原则1.正样本和负样本的数量应该大体平衡；2.困难的负样本应该被更加频繁地采样（困难的负样本：分类分数比较高而与GT的overlap比较低，两个tubelets的overlap通过平均K帧的IoU获得。），首先给定某个step的overlap的阈值，把和gt重合率最高的proposal指定为正样本，燃火是剩下的比阈值高的proposal加入到正样本的pool，剩下的加入负样本的pool，然后按照分类分数比例去从两个pool当中sample样本。


Conclusion: very solid work，不知从哪开始follow :)
