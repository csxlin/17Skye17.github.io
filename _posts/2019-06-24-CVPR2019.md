---
title: 'CVPR2019 Paper Reading'
date: 2019-06-24
permalink: /posts/2012/08/blog-post-13/
tags:
  - CVPR 2019
---

+ **From Recognition to Cognition: Visual Commonsense Reasoning**

提出了一个VQA数据集VCR，和一个Recognition to Cognition Networks(R2C)模型，主要包括三个部分：

1. Grounding

学习图片和语言的联合表示，图片的object-level特征由CNN得到，语言特征由BERT得到，然后把两个特征输入双向LSTM。

2. Contextualization　语境

使用attention机制来联系句子和图片上下文语境

3. Reasoning

将query, response和物体的attention表示输入一个双向LSTM，每一个time step将LSTM的输出和问答的表示连接，最后通过max-pooling和MLP输出QA的匹配分数。

实验在VCR数据集上测试了一些SOTA模型和R2C模型，R2C模型比现有模型超出大约20%，看Appendix里关于GloVe和BERT的对比结果，BERT提供了大约20-30%的提升，如果不采用BERT与现有模型相比只有2%-4%的提升。So，涉及到Visual-Language问题，无脑BERT...

不过看模型VQA问题还是很大杂烩= =

+ **Relational Action Forecasting** (Carl Vondrick大佬的work，16年NIPS VGAN入坑)

提出DR2N捕捉视频当中不同entity的关联来做空间时间推理。在自己组里ECCV2018的一个工作ACRN(AActor-centric Relation Network)的基础上改进来预测未来的动作标签。

相关reasoning工作很多，Interaction Network(IN), Relation Network(RN), Graph Neural Network(GNN), Graph ATtention networks,应用到video上的有：ACRN(ECCV2018),Object Relation Network(ORN, ECCV2018)

+ **Classification-Reconstruction Learning for Open-Set Recognition** (Non-local作者Xiaolong Wang和Allan Jabri合作)

采用时间上的周期一致性作为监督信号，从头学习视觉表示。在训练期间，模型学习一个跟踪周期一致性的feature map表示；测试期间，采用所需的表示来找时空上的最近邻。在不需要finetune的情况下，在video object segmentation, keypoint tracking和optical flow上性能大大超过其他自监督方法。

+ **Gaussian Temporal Awareness Networks for Action Localization** (梅涛，罗杰波组)

