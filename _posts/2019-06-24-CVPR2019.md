---
title: 'CVPR2019 Paper Reading'
date: 2019-06-24
permalink: /posts/2012/08/blog-post-13/
tags:
  - CVPR 2019
---

+ **From Recognition to Cognition: Visual Commonsense Reasoning**

提出了一个VQA数据集VCR，和一个Recognition to Cognition Networks(R2C)模型，主要包括三个部分：

1. Grounding

学习图片和语言的联合表示，图片的object-level特征由CNN得到，语言特征由BERT得到，然后把两个特征输入双向LSTM。

2. Contextualization　语境

使用attention机制来联系句子和图片上下文语境

3. Reasoning

将query, response和物体的attention表示输入一个双向LSTM，每一个time step将LSTM的输出和问答的表示连接，最后通过max-pooling和MLP输出QA的匹配分数。

实验在VCR数据集上测试了一些SOTA模型和R2C模型，R2C模型比现有模型超出大约20%，看Appendix里关于GloVe和BERT的对比结果，BERT提供了大约20-30%的提升，如果不采用BERT与现有模型相比只有2%-4%的提升。So，涉及到Visual-Language问题，无脑BERT...

不过看模型VQA问题还是很大杂烩= =

+ **Relational Action Forecasting** (Carl Vondrick大佬的work，16年NIPS VGAN入坑)

提出DR2N捕捉视频当中不同entity的关联来做空间时间推理。在自己组里ECCV2018的一个工作ACRN(AActor-centric Relation Network)的基础上改进来预测未来的动作标签。

相关reasoning工作很多，Interaction Network(IN), Relation Network(RN), Graph Neural Network(GNN), Graph ATtention networks,应用到video上的有：ACRN(ECCV2018),Object Relation Network(ORN, ECCV2018)

+ **Classification-Reconstruction Learning for Open-Set Recognition** (Non-local作者Xiaolong Wang和Allan Jabri合作)

采用时间上的周期一致性作为监督信号，从头学习视觉表示。在训练期间，模型学习一个跟踪周期一致性的feature map表示；测试期间，采用所需的表示来找时空上的最近邻。在不需要finetune的情况下，在video object segmentation, keypoint tracking和optical flow上性能大大超过其他自监督方法。

+ **Gaussian Temporal Awareness Networks for Action Localization** (梅涛，罗杰波组)

解决的问题：目前大多数时序动作定位的算法是用的图片当中物体检测算法，例如使用SSD和Faster R-CNN来产生一个动作的时序位置。由于时间尺度是预定的，目前的方法不能很好地检测一些复杂的动作而且不够robust。

提出的方法：采用Gaussian kernel来动态地优化每一个动作proposal的时序尺度（每个动作时长不一），提出了Gaussian Temporal Awareness Networks(GTAN). ，每个高斯核对应一个动作proposal的特定的一段，混合高斯核能够进一步以不同长度表征动作proposal，每一个高斯曲线的值能够反应对一个动作proposal定位的上下文贡献。
模型先由多个3D卷积提取一些video clips的特征map，然后输入级联的1D Conv产生多个不同时序尺度的特征map，然后每个高斯核学习控制一个action proposal的时间尺度，最后多个高斯核通过grouping成一个更大的高斯核，总的loss是动作分类loss+定位loss＋高斯核重合loss。
