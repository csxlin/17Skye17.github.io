---
title: 'CVPR2019 Paper Reading'
date: 2019-06-24
permalink: /posts/2012/08/blog-post-13/
tags:
  - CVPR 2019
---

+ **From Recognition to Cognition: Visual Commonsense Reasoning**

提出了一个VQA数据集VCR，和一个Recognition to Cognition Networks(R2C)模型，主要包括三个部分：

1. Grounding

学习图片和语言的联合表示，图片的object-level特征由CNN得到，语言特征由BERT得到，然后把两个特征输入双向LSTM。

2. Contextualization　语境

使用attention机制来联系句子和图片上下文语境

3. Reasoning

将query, response和物体的attention表示输入一个双向LSTM，每一个time step将LSTM的输出和问答的表示连接，最后通过max-pooling和MLP输出QA的匹配分数。

实验在VCR数据集上测试了一些SOTA模型和R2C模型，R2C模型比现有模型超出大约20%，看Appendix里关于GloVe和BERT的对比结果，BERT提供了大约20-30%的提升，如果不采用BERT与现有模型相比只有2%-4%的提升。So，涉及到Visual-Language问题，无脑BERT...

不过看模型VQA问题还是很大杂烩= =

+ **Relational Action Forecasting** (Carl Vondrick大佬的work，16年NIPS VGAN入坑)

