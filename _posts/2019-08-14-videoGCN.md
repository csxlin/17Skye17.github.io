---
title: 'Video+GCN'
date: 2019-08-07
permalink: /posts/2012/08/blog-post-21/
tags:
  - Action Recognition
  - Video Classification
  - Relation Reasoning
  - GCN
---

**(ECCV2018)Videos as Space-Time Region Graphs** (Xiaolong Wang)

用GCN来建模视频动作识别：　

1.建模相关物体长范围的相似性关联；

2.建模相邻物体的时空关联。

大致方法：将输入的视频帧当做一个space-time region graph，其中每个node表示视频中的ROI，node之间有两种edge:１.外观相似度; 2.时空近似。
总的来说是在原始的i3d特征之上，再融合了一个object clue，用GCN编码object clue，总体上是global+local特征。

重点：编码object，用了一个similarity graph和一个spatial-temporal graph。
其中similarity graph的edge用一个邻接矩阵表示，任意i和j物体之间的相似性计算由object proposal feature之间的亲和性表示（将各自的feature先做embedding再相乘）。再对矩阵的每一行作softmax进行归一化。

spatial-temporal graph是一个DAG（双向），用一个邻接矩阵表示边权。边权由两个object bbox的IoUs表示，然后所有的边权进行l1 normalization。

构造好graph之后，在graph作卷积操作进行推理，具体操作是分为两个branch（主要应为similarity grpah中要学习的两个embedding矩阵参数，DAG中没有学习参数，如果直接把这三个graph直接先concatenate，再share图卷积的训练参数，会导致训练不稳定结果变差）：

1. similarity graph单独作为一个branch，与输入的Nxd维i3d特征相乘再乘以一个dxd维的模型参数矩阵。

2. DAG作为一个branch，front graph和back graph的邻接矩阵先单独进行如1的矩阵乘操作再concatenate。

最后把两个branch输出加上一个i3d特征的残差得到最终的feature。


训练步骤比较繁琐：

1.先在ImageNet上预训练后的i3d上用kinetics来finetune（固定bn参数）；

2.固定提取i3d特征的网络参数，只训练GCN；

3.端到端训练i3d提特征网络参数和GCN网络参数。


实验在charades和something-something数据集上进行了测试，这两个数据集公开结果比较少，论文中仅分别引用了4篇和2篇论文中的数据，算是xiaolong wang接着non-local network的一个工作延伸。（期待大佬的thesis）

问题：
1. 总的object的个数N应该是多少？（一个数据集中可能object有成千上万个，这样在图卷积操作时的邻接矩阵和输入特征矩阵的维度非常大10,000 x 10,000）
2. 有没有能够直接end-to-end training的方案？

ps：论文中画图用的工具应该是seaborn（matplotlib精美封装版本），看起来很炫酷。
